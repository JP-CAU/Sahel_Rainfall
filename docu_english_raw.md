## Introduction
### Task
The primary objective of the project was to achieve highly accurate predictions of rainfall in Africa's Sahel region. These predictions were required for various lead times, with "lead time" denoting the duration between when a forecast is generated and when it becomes pertinent or is put into use within the context of time series forecasting. The specific lead times of interest in this project were zero, one, three, and six.

To carry out these predictions, a foundational dataset was provided as the basis for the forecasting models.

### Dataset
The dataset in use is the CICMOD dataset, which comprises 1000 years of synthetic climate data generated by cutting-edge climate models, namely FOCI and CESM. This dataset is multivariate, containing 29 distinct climate indices, each recorded on a monthly basis. Among these indices is the measurement of rainfall in the Sahel region. To be precise, there are observations available at 12,000 distinct time points, with each point encompassing data on all 29 climate indices.

For computational reasons, the dataset provides 1000 years of data for the CESM model and 999 years for the FOCI model.

### Data preprocessing
The time series data from the CICMOD dataset underwent normalization to achieve zero mean and unit variance for all climate indices. Given the synthetic nature of the data, which was devoid of typical issues like missing values, no further preprocessing was required.

Regarding the training data, it varied depending on the lead time being considered. For Lead Time zero, the Sahel Rainfall Index was excluded from the training dataset. The rationale behind this decision was that, at Lead Time zero, the model already possessed knowledge of the actual rainfall value and could straightforwardly output it. Conversely, for all other lead times, the complete dataset comprising all 29 indices was utilized.

A significant portion of the machine learning models examined in this project operates with a window of past values as their input. This window's size dictates the number of previous values considered when making a current prediction. For instance, a window size of twelve signifies that the model predicts rainfall based on data from the past year. As part of the data preprocessing, the dataset was divided into these respective windows.

In the final phase of the project, various optimization measures were carried out to achieve an optimal result. One of these measures was feature selection. In this step, a selected subset of the indexes was used. The window length was also optimized in this step.

## Model selection
At the beginning of the project, there was a comprehensive discussion to gather recommendations on which models might be appropriate for predicting Sahel region rainfall. This collaborative effort led to the identification of the following models:
- (linear) Regression
- Multi Layer Perceptron
- Convolutional Neural Network
- Recurrent Neural Network (with Attention)
- Long Short Term Memory (with Attention)
- Hybrid CNN + LSTM
- Gated Recurrent Unit Network
- Echo State Networks
- Random Forest
- ADA Boost
- XG Boost
- Light GBM

### Train/Test Split
The specified models were each trained on 80 percent (~800 years) of the data set. The remaining 20 percent was used equally for validation and testing. In total, 80 percent was used for training, 10 percent for validation, and 10 percent for testing. The data was not shuffled before splitting, so the first 800 years were used for training, the next 100 years for validation, and the last 100 years for testing.

### Baseline results
In the first step, each of the models described above was trained. In order to establish comparability, no optimization was performed for any of the models. The configuration of the hyperparameters corresponded either to the default of the appropriate Python libraries or was set based on empirical values. 
The goal of this phase was to identify the models that perform best in order to optimize them in a second step. 
The results of the baseline models are shown in Table 1:

## Optimization
After the initial selection of models, the most promising models were further optimized.

Possible areas of optimization were:
- Feature selection (selection of a subset of features)
- Adjustment of the input window length
- Hyperparameter tuning
- Feature Engineering (creation of new features)

At the beginning it quickly became clear that the above optimization options influence each other. For example, the relevance of individual features, which is the basis for feature selection, is strongly dependent on the input window length (see figure). The order of the optimization steps thus plays a role. The optimization procedure was different.

### Optimization Jake
Optimized models:
- GRU
- LSTM
- CNN + LSTM
- XG Boost

The initial step involved feature engineering with the objective of enhancing the model's forecasting performance. Various novel features were experimented with, and their impact on both climate models and all lead times was evaluated. Due to the time-intensive nature of this step, performance evaluations were exclusively conducted on the most promising model type, namely LSTM. It's worth noting that the results might have differed if applied to other model types.

The first new feature introduced one-hot encoding for the month of the year (1-12), resulting in 12 binary features where each binary digit represents a specific month. For instance, January would be represented as "100000000000." This feature notably improved the average performance of the LSTM model.

The second new feature represented the month of the year (1-12) using cosine encoding, mapping each month's numerical value onto the cosine function. This encoding method preserves the cyclical nature of months, where December (12) should be close to January (1), unlike a simple numerical representation. However, this feature exhibited a comparatively smaller improvement in the LSTM model's average performance when compared to one-hot encoding.

Lastly, a group of three additional features was introduced as part of this feature engineering process.
- The first feature calculates the average of the values from the last three months. This is achieved by computing the mean of the data for the most recent three months.

- The second feature calculates the average of the values for the current month over the last three years. It entails taking the mean of the data for the same month from the previous three years.

- The last feature simply represents the value of the most recent month.

Surprisingly, none of these additional features led to an improvement in the performance of the LSTM model. Consequently, the feature engineering process resulted in the decision to only incorporate the month of the year as a one-hot encoding feature for enhanced model input.

Subsequent steps were executed sequentially for each of the individual models. The first step involved feature selection. For the neural network models, feature selection was performed using a backward elimination technique. In this method, known as backward selection, the goal is to reduce the number of input features used in the neural network model. It commences with all available features and progressively eliminates one feature at a time until a predefined stopping criterion is met. In this specific scenario, only the removal of one of the 29 features at a time was assessed. When the removal of a feature resulted in improved model performance, that feature was identified as an interfering feature and subsequently excluded from the training dataset.

Conversely, for tree-based models like XG Boost, the feature importance metric was leveraged to differentiate between valuable and less valuable features in the dataset.

The next step involved adjusting the input window size, also referred to as the sequence length. A range of sequence lengths, spanning from 2 to 24, were systematically tested. Interestingly, the optimal sequence length varied for each individual model. Initially, there was an assumption that multiples of 12 might yield the best results; however, this hypothesis was disproven by the experimental findings.

Following the adjustment of input window sizes, the next critical step was hyperparameter tuning. For the neural network (NN) approaches, the KerasTuner API was employed to carry out this process. Using this API, a grid search was conducted within a predefined space of possible hyperparameter values. The key hyperparameters that underwent optimization included the learning rate, the number of units in each layer of the neural network, the total number of layers, and, in the case of CNNs, the number of filters and their respective sizes.

Remarkably, the results of hyperparameter tuning revealed minimal to negligible differences when compared to the initial hyperparameter settings. This outcome suggests that the initial empirical values may have already represented a proficient configuration.

However, it's worth noting that due to time constraints, hyperparameter tuning was not carried out for the tree-based approaches.

### Optimization Jannik

## Results
The results reveal that optimization yielded benefits for most configurations. However, due to substantial variations in performance driven by factors like lead time, sequence length, and climate model choice, the models did not uniformly enhance their performance across all configurations. Interestingly, improvements in performance for the FOCI climate model occasionally corresponded to decreases in performance for the CESM climate model, and vice versa. The final results can be found in table 2.
