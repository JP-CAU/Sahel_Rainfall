# Research Question
Are machine learning models able to precisely predict the rainfall in the Sahel region in africa?

## Objective
The primary objective of the project was to achieve highly accurate predictions of rainfall in Africa's Sahel region. These predictions were required for various lead times, with "lead time" denoting the duration between when a forecast is generated and when it becomes pertinent or is put into use within the context of time series forecasting. The specific lead times of interest in this project were zero, one, three, and six.

To carry out these predictions, a foundational dataset was provided as the basis for the forecasting models.

## Dataset
The dataset in use is the CICMOD dataset, which comprises 1000 years of synthetic climate data generated by cutting-edge climate models, namely FOCI and CESM. This dataset is multivariate, containing 29 distinct climate indices, each recorded on a monthly basis. Among these indices is the measurement of rainfall in the Sahel region. To be precise, there are observations available at 12,000 distinct time points, with each point encompassing data on all 29 climate indices.

For computational reasons, the dataset provides 1000 years of data for the CESM model and 999 years for the FOCI model.

## Data preprocessing
The time series data from the CICMOD dataset underwent normalization to achieve zero mean and unit variance for all climate indices. Given the synthetic nature of the data, which was devoid of typical issues like missing values, no further preprocessing was required.

Regarding the training data, it varied depending on the lead time being considered. For Lead Time zero, the Sahel Rainfall Index was excluded from the training dataset. The rationale behind this decision was that, at Lead Time zero, the model already possessed knowledge of the actual rainfall value and could straightforwardly output it. Conversely, for all other lead times, the complete dataset comprising all 29 indices was utilized.

A significant portion of the machine learning models examined in this project operates with a window of past values as their input. This window's size dictates the number of previous values considered when making a current prediction. For instance, a window size of twelve signifies that the model predicts rainfall based on data from the past year. As part of the data preprocessing, the dataset was divided into these respective windows.

In the final phase of the project, various optimization measures were carried out to achieve an optimal result. One of these measures was feature selection. In this step, a selected subset of the indexes was used. The window length was also optimized in this step.

# Model Selection

At the beginning of the project, there was a comprehensive discussion to gather recommendations on which models might be appropriate for predicting Sahel region rainfall. This collaborative effort led to the identification of the following models:
- (linear) Regression
- Multi Layer Perceptron
- Convolutional Neural Network
- Recurrent Neural Network (with Attention)
- Long Short Term Memory (with Attention)
- Hybrid CNN + LSTM
- Gated Recurrent Unit Network
- Echo State Networks
- Random Forest
- ADA Boost
- XG Boost
- Light GBM

## Train/Test Split
The specified models were each trained on 80 percent (~800 years) of the data set. The remaining 20 percent was used equally for validation and testing. In total, 80 percent was used for training, 10 percent for validation, and 10 percent for testing. The data was not shuffled before splitting, so the first 800 years were used for training, the next 100 years for validation, and the last 100 years for testing.

# How was the project solved

## Optimization
After the initial selection of models, the most promising models were further optimized.

Possible areas of optimization were:
- Feature selection (selection of a subset of features)
- Adjustment of the input window length
- Hyperparameter tuning
- Feature Engineering (creation of new features)

At the beginning it quickly became clear that the above optimization options influence each other. For example, the relevance of individual features, which is the basis for feature selection, is strongly dependent on the input window length (see figure). The order of the optimization steps thus plays a role. The optimization procedure was different.

### Optimization Jake
Optimized models:
- GRU
- LSTM
- CNN + LSTM
- XG Boost
- Light GBM

The initial step involved feature engineering with the objective of enhancing the model's forecasting performance. Various novel features were experimented with, and their impact on both climate models and all lead times was evaluated. Due to the time-intensive nature of this step, performance evaluations were exclusively conducted on the most promising model type, namely LSTM. It's worth noting that the results might have differed if applied to other model types.

The first new feature introduced one-hot encoding for the month of the year (1-12), resulting in 12 binary features where each binary digit represents a specific month. For instance, January would be represented as "100000000000." This feature notably improved the average performance of the LSTM model.

The second new feature represented the month of the year (1-12) using cosine encoding, mapping each month's numerical value onto the cosine function. This encoding method preserves the cyclical nature of months, where December (12) should be close to January (1), unlike a simple numerical representation. However, this feature exhibited a comparatively smaller improvement in the LSTM model's average performance when compared to one-hot encoding.

Lastly, a group of three additional features was introduced as part of this feature engineering process.
- The first feature calculates the average of the values from the last three months. This is achieved by computing the mean of the data for the most recent three months.

- The second feature calculates the average of the values for the current month over the last three years. It entails taking the mean of the data for the same month from the previous three years.

- The last feature simply represents the value of the most recent month.

Surprisingly, none of these additional features led to an improvement in the performance of the LSTM model. Consequently, the feature engineering process resulted in the decision to only incorporate the month of the year as a one-hot encoding feature for enhanced model input.

Subsequent steps were executed sequentially for each of the individual models. The first step involved feature selection. For the neural network models, feature selection was performed using a backward elimination technique. In this method, known as backward selection, the goal is to reduce the number of input features used in the neural network model. It commences with all available features and progressively eliminates one feature at a time until a predefined stopping criterion is met. In this specific scenario, only the removal of one of the 29 features at a time was assessed. When the removal of a feature resulted in improved model performance, that feature was identified as an interfering feature and subsequently excluded from the training dataset.

Conversely, for tree-based models like XG Boost, the feature importance metric was leveraged to differentiate between valuable and less valuable features in the dataset.

The next step involved adjusting the input window size, also referred to as the sequence length. A range of sequence lengths, spanning from 2 to 24, were systematically tested. Interestingly, the optimal sequence length varied for each individual model. Initially, there was an assumption that multiples of 12 might yield the best results; however, this hypothesis was disproven by the experimental findings.

Following the adjustment of input window sizes, the next critical step was hyperparameter tuning. For the neural network (NN) approaches, the KerasTuner API was employed to carry out this process. Using this API, a grid search was conducted within a predefined space of possible hyperparameter values. The key hyperparameters that underwent optimization included the learning rate, the number of units in each layer of the neural network, the total number of layers, and, in the case of CNNs, the number of filters and their respective sizes.

Remarkably, the results of hyperparameter tuning revealed minimal to negligible differences when compared to the initial hyperparameter settings. This outcome suggests that the initial empirical values may have already represented a proficient configuration.

However, it's worth noting that due to time constraints, hyperparameter tuning was not carried out for the tree-based approaches.

### Optimization Jannik
Optimized models:
 - (attention) RNN
 - (attention) LSTM
 - adaboost
 - random forest

To have reliable results the random seed was fixed for each experiment.

The initial models were rather small as the hope was to reduce the overfitting, however this lead to problems during higher lead times where the models would barely learn if at all.
Due to this the model size was increased until the models managed to handle larger lead times before any other optimization was even tested.

The first step for the optimization of the models was the feature selection. 
For this a random forest was trained and its feature ranking extracted. Since for each month that was used as an input contained 28 (or 29 for lead times bigger than 0) features, the importance of a feature would be summed up across all months to obtain a ranking of features. Additionally all features of a month would be summed up to obtain the importance for each month in the sequence. 
Changing the lead time or the sequence length also changed the ranking of the features. Different lead times also had different importances for months in the sequence. In the case of lead time 0 the current month was by far the most important one, while for larger lead times the other months slowly increased in their importance.
Due to the different rankings based on changes in sequence length and lead time, a sequence length was determined for each lead time by training a model for a given set of sequence lengths and evaluating it using its MSE.
After this forward, as well as backward selection was tested. Forward selection outperformed backward selection at a lead time of 0 while also having a faster runtime, so it was chosen going forward. All features were used if the selection did not improve the performance of the model.
Additionally it was decided to select features based on the correlation coefficient instead of MSE. This was due to a model with a lower MSE sometimes having a worse correlation coefficient. 
Lastly after testing the selected features on different models (RNN and LSTM namely) it could be observed, that the selection lowered their performance. However their performance could be improved by performing the feature selection using the respective model.
The main results of the feature selection were the following:
 - for each lead time a sequence length has to be determined first
 - then for each lead time the features can be selected based on the correlation coefficient
 - selection was done using forward selection
 - selected features of one model do not necessarly work for another model

 Since adding the month to the features seemed to improve the performance different encodings of the month were tested: month as integer, one hot encoded, one hot encoded and scaled to have a mean of 0 and std of 1. As with the feature selection different models and different lead times influenced how the different encodings performed. This again leaves one with the informatino, that:
  - the optimal encoding depends on the lead time and model

To search for fitting hyper parameters ray tune was utilized to perform a grid search. 
At first the grid search also contained multiple activation functions, namely ReLU, LeakyReLU and Tanh, however since ReLU always ended up performing best in later grid searches only ReLU was used. The number of neurons and number of layers were searched for while some hyper parameters werde fixed and using all features. The sequence length was fixed, the learning rate was set to 0.001 and a weight decay of 0 was used. The grid search was performed using a lead time of 1, it most likely would have been better to search for a different set of hyper parameters for each lead time, however this would have also taken a fairly long time.
After the first grid search a second one was used to determine the learning rate and weight decay, however the best values seemed to be the initial ones which could be due to the other hyper parameters being chosen depending on them. Nonetheless two grid searches instead of one were performed to keep the required time low. Dropout was not searched for as in previous tests it did not seem to increase performance. For the attention networks it was also tested how many attention heads should be used. 

The resulting optimization pipeline was as follows:
First the grid search was performed, followed by the search of a fitting sequence length for every lead time. Afterwards forward selection was performed for every lead time followed by finding the best encoding. Lastly the final evaluation was done.

During optimization models were always evaluated on the validation set and only the final evaluation utilized the test set.

Additionally a larger validation set was testen on the lstm for lead times 0 and 1, which improved the performance for lead time 1.
Pre training on one of the two data sets was also tested for a lead time of 1. For FOCI this ended up in as a rather large correlation coefficient improvement, however it slightly decreased the performance for CESM.

## Optimization
After the initial selection of models, the most promising models were further optimized.

Possible areas of optimization were:
- Feature selection (selection of a subset of features)
- Adjustment of the input window length
- Hyperparameter tuning
- Feature Engineering (creation of new features)

### Model Selection: 
Jake focused on models like GRU, LSTM, CNN + LSTM, XG Boost, and Light GBM, while Jannik explored (attention) RNN, (attention) LSTM, AdaBoost, and Random Forest.

### Feature Selection:
Jannik used foward selection to identify the best performing features while Jake tried to identify features that hurt performance by implementing backward elimination.

### Sequence length:
The length of the input window was found to have a large impact on model performance, so the sequence length was optimized for each of the models.

### Hyperparameter Tuning: 
To find the optimal configuration of hyperparameters, a grid search approach was implemented. Jake employed KerasTuner for neural networks, and Jannik used Ray Tune for hyperparameter optimization.

### Feature Engineering: 
For feature engineering both tried different encodings of months (integer, one-hot, scaled one-hot) and found the optimal encoding to depend on the lead time and model. Jake experimented with various additional features like rolling averages. 

After conducting several experiments both approaches used the month of year as one-hot encoding as the only additional feature.

Models were evaluated primarily on validation sets during the optimization, with a final evaluation on the test set.

Other Considerations: Jannik experimented with larger validation sets and pre-training on specific datasets, which yielded improvements for certain lead times and models.

Overall, both approaches followed systematic optimization pipelines to improve model performance, including feature selection, sequence length, hyperparameter tuning, and feature engineering, with careful consideration of lead times and model differences.

# Challenges
As mentioned above during the optimization phase a major challenge emerged. All of the the identified optimization areas influence each other. As a result only a specific 

## Results
The results reveal that optimization yielded benefits for most configurations. However, due to substantial variations in performance driven by factors like lead time, sequence length, and climate model choice, the models did not uniformly enhance their performance across all configurations. Interestingly, improvements in performance for the FOCI climate model occasionally corresponded to decreases in performance for the CESM climate model, and vice versa. As expected larger lead times were harder to predict. The final results can be found in table 2.

# Project Plan

At the project's outset, a well-defined milestone plan was established with the initial objectives of identifying promising machine learning models and establishing a performance baseline. In the subsequent phase of the project, the focus shifted towards optimizing the most promising models. Although the incorporation of explainable AI was designated as an optional task, it was ultimately deemed unattainable within the project's time constraints. Other than that the project adhered closely to its outlined plan.

# Learnings
What would we do differently next time:
- Stick to best practices to keep the github repository as clean as possible. During the project the github repositories started to get messy when running more and more experiences. Refactoring the code and cleaning up the repository would be a lot of work now. Next time we would stick to clear conventions.